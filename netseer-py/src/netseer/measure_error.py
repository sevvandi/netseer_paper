from typing import Tuple


def measure_error(actual, predicted) -> Tuple[int, int]:
    """Returns a tuple containing the Node and Edge error rates, comparing an Atual/Ground Truth Graph to a Predicted Graph.

    Args:
        actual (iGraph): An Actual Graph to be compared to a Predicted Graph.
        predicted (iGraph): A Predicted Graph generated by prediction.predict_graph().
    """
    n_error = node_error(actual, predicted)

    e_error = edge_error(actual, predicted)

    return n_error, e_error


def node_error(actual, predicted):
    num_nodes_actual = actual.vcount()
    num_nodes_predicted = predicted.vcount()
    return (num_nodes_predicted - num_nodes_actual) / num_nodes_actual


def edge_error(actual, predicted):
    num_edges_actual = actual.ecount()
    num_edges_predicted = predicted.ecount()
    return (num_edges_predicted - num_edges_actual) / num_edges_actual
